加速计算正在取代CPU计算，成为目前最佳的计算方法，其中通过GPU进行加速又是在目前一种主流的加速计算方式。在本次实验中，就基于NVIDIA的GPU学习平台，学习了有关CUDA的一些基本的原理和使用技巧。

CUDA计算平台，提供了一种可以扩展C、C++、Python和Fortran等语言的编码范式，该凡是能够在世界上性能超强的并行处理器NVIDIA GPU上运行经过加速的大规模并行代码。CUDA可以毫不费力地大幅度加速应用程序，具有适用于DNN、BLAS、图形分析和FFT等更多运算的高度优化库生态系统，并且还附带了强大的命令行和可视化性能分析工具。

## 使用CUDA C/C++加速应用程序

### 学习目标

* 编写编译及运行既可以调用CPU函数又可以启动GPU核函数的C/C++程序
* 使用执行配置控制并行线程层次结构
* 重构串行循环以在GPU上并行执行迭代
* 分配和释放可用于CPU和GPU的内存
* 处理CUDA代码生成的错误
* 加速CPU应用程序

### 学习及实验

加速系统又称异构系统，由 CPU 和 GPU 组成。加速系统会运行 CPU 程序，这些程序会调用能够利用GPU进行并行优化加速的核函数。一般的，并不会把程序的所有过程全部部署到GPU上进行运算，原因是GPU上更适合一些大规模的算术运算，而对于逻辑判断等运算并没有显著优势，并且还可能性能差于CPU。因此需要在合适的地方将计算过程通过核函数的方式分发给GPU，进行加速优化。

#### 核函数

如果想要让某个函数能够分配到GPU上执行，需要首先将这个函数声明为核函数，即使用`__global__`关键字，这个关键字表明以下函数将在GPU上运行，并且可以又CPU或GPU调用。通常的，将CPU上执行的代码成为主机代码，而将GPU上执行的代码成为设备代码。

而在调用和函数的时候，需要使用一种特殊的语法标记`GPUFunction<<<number_of_blocks,threads_per_block>>>()`，这表明，启用了number_of_blocks个线程块，而每个线程块内包含threads_per_block，这个函数将会被分配到所有的线程上去并行执行。CUDA的线程层次结构如下图所示

![image-20220613213324039](E:\project\parallel\Lab6\report\image-20220613213324039.png)

同时，由于核函数的启动方式是异步的，CPU代码将不会等待GPU上的程序完全执行完成后再开始。因此在调用了核函数之后，一般需要进行一次同步，即需要等待所有的在GPU上执行的函数完成后，才能够继续串行去执行后续的代码。这就需要使用到`cudaDeviceSynchronize()`函数。

#### CUDA线程层次结构变量

在上一小节中曾经介绍了CUDA的线程层次，即是通过线程块来进行组织的。每一个核函数可以使用多个线程块，并且可以指定线程块内线程的数量。因此我们就需要有一些变量，能够在函数内部确定当前的函数是在哪一个线程块中的哪一个线程上执行的，于是就提出了线程层次结构变量。

每个线程在其线程块内部均会被分配一个索引，从0开始。此外，每个线程块也会被分配一个索引，并从0开始。正如线程组成线程块，线程块又会组成网格，而网格是 CUDA 线程层次结构中级别最高的实体。简言之，CUDA 核函数在由一个或多个线程块组成的网格中执行，且每个线程块中均包含相同数量的一个或多个线程。

CUDA 核函数可以访问能够识别如下两种索引的特殊变量：正在执行核函数的线程（位于线程块内）索引和线程所在的线程块（位于网格内）索引。这两种变量分别为 `threadIdx.x` 和 `blockIdx.x`。

#### 加速for循环

对于一个循环而言，并非要顺次运行循环的每次迭代，而是让每次迭代都在自身线程中并行运行。因此必须首先编写循环的单次迭代工作的核函数，由于核函数于其他正在运行的核函数无关，因此执行配置必须使用核函数执行正确的次数。

当然由于每一个线程块内的线程数量是有一个上限的，因此很有必要在调用核函数的时候，同时启用多个线程块，并且在每个线程块内启用多个线程。这时，就可以通过 `threadIdx.x` 和 `blockIdx.x`来定位当前线程在整个任务中的一个为止，并根据此计算出当前线程所负责的任务。

接下来的练习就是使用多个线程块来加速for循环，由于各个线程之间是并行的，因此数字0-9的输出很有可能是乱序的。

#### CPU和GPU内存分配

CUDA的最新版本已经能够轻松的分配可用于CPU主机和任意数量GPU设备的内存。尽管现在有许多适用于内存管理并且可支持加速应用程序中最优性能的，但是现在要介绍的基础CUDA内存管理技术还是不能支持远超CPU应用程序的卓越性能。

其主要区别就是在申请内存的时候，使用`cudaMallocMananged`函数，并在释放内存的时候，使用`cudaFree`函数。

在实验中，要求尝试能够分配一种既能在CPU上访问又能在GPU上访问的内存，并且对于这个数组进行一个翻倍的操作。因此只需要使用`cudaMallocManaged`和`cudaFree`函数进行内存的申请和释放，并且将loop函数重构为核函数即可。

#### 数据与网格不匹配

当然，由于数据的规模不同，很有可能会出现数据规模与网络大小不匹配的现象，包括各个网格分配的任务数量不同，以及数据比网格大的时候需要采用循环划分的方式。

那么首先就是要注意，需要在每个线程执行核函数的时候判断当前的任务标号是否是合法的，并且如果数据集比网格的大的时候，还会涉及到跨网格的数据访问，这就类似于之前在进行任务划分时的循环任务划分的方式。这时只需要注意修改一下循环的步幅即可，将之前的步幅修改为`gridDim.x * blockDim.x`，而在每个线程内仍采用之前的寻址方式。

![image-20220613223730354](E:\project\parallel\Lab6\report\image-20220613223730354.png)

#### 错误处理

对于任何应用程序而言，错误的检查和处理都是至关重要的，加速CUDA代码中同样有错误处理的相关操作，并且CUDA的很多内置函数都以`cudaError_t`为返回值类型，该返回值可以用于检查调用函数的时候是否发生了错误。但是由于核函数必须声明为`void`类型，因此就不能够通过返回值来判断是否发生了错误，可以使用CUDA提供的方法`cudaGetLastError`方法获取最近的一次错误信息。对于内存申请，以及核函数执行之后的同步操作都需要使用`cudaError_t`的返回值来检查是否发生了错误。

## CUDA C/C++统一内存

### 学习目标

* 使用 Nsight Systems 命令行分析被加速的应用程序的性能
* 利用对流多处理器的理解优化执行配置
* 理解统一内存在也错误和数据迁移方面的行为
* 使用异步内存预取减少页错误和数据迁移以提高性能
* 采用循环式的迭代开发加速应用恒旭的优化加速和部署

### 学习和实验

#### 使用nsys性能分析工具

nsys时NVIDIA的命令行分析器，提供分析被加速的应用程序性能的强大功能。nsys会执行使用nvcc编译的可执行程序，并打印应用程序的GPU活动的摘要输出、CUDA API调用情况以及同意内存活动的相关信息。

使用`nsys profile` 将会生成一个`qdrep`报告文件，可以增加指令选项`--stats=true`打印输出摘要信息。

#### 流多处理器

运行CUDA应用程序的GPU具有成为流多处理器（SM）的处理单元，在执行核函数的期间，将线程块提供给SM以供其执行。为支持GPU执行尽可能多的并行操作，通常可以选择线程块的数量数倍于指定的GPU上SM数量的网格大小来提高性能。并且SM会在一个名为warp的线程块内创建、管理、调度和执行包含32个线程的线程组，因此可以选择线程数量数倍于32的线程块大小来提升性能。

![image-20220614213733118](E:\project\parallel\Lab6\report\image-20220614213733118.png)

在CUDA中提供了API能够获取SM的数量，即可以调用函数`cudaDeviceGetAttribute`，指定获取`cudaDevAttrMultiProcessorCount`属性的值，便可以得到流多处理器的数量。之后在声明线程的数量的时候，就可以将其数量指定为SM的数倍。

#### 统一内存

通常的使用函数`cudaMallocMananged`分配旨在供主机或设备代码使用的内存，这种方法的便利是能够自动实现内存的迁移并简化编程，但同样会带来内存迁移而产生的性能损失。因此有必要对CUDA的统一内存进行了解。

如果是在CPU上调用了`cudaMallocManaged`函数，则所申请的内存会出现在CPU上，而当GPU想要访问相关的数据的时候，就需要将这部分内存分配的GPU上。而如果在GPU上的核函数调用了`cudaMallocManaged`函数，则申请的内存会分配在GPU上，因此当CPU想要访问这部分数据的时候，又要将数据从GPU迁移的CPU上。

#### 异步内存预取

在分配UM的时候，最初可能并没有驻留在CPU或者GPU上，因此当某些线程执行工作的时候就可能发生页错误，因此就会触发内存的迁移，这会造成性能的损失。

在主机到设备和设备到主机的内存传输过程中，我们使用异步内存预取的技术来减少页错误和按需内存迁移成本。通过此技术，可以在应用程序代码使用统一内存 (UM) 之前，在后台将其异步迁移至系统中的任何 CPU 或 GPU 设备。此举可以减少页错误和按需数据迁移所带来的成本，并进而提高 GPU 核函数和 CPU 函数的性能。

但是由于预取往往会以更大的数据块来迁移数据，因此这种迁移的成本也是很高的，也要尽可能减少这种预取操作。一般的，当在运行之前已经知道数据访问需要且数据访问并未采用稀疏模式的时候，就可以考虑使用异步内存预取。

CUDA也提供了相关的函数来实现异步内存预取，可以使用`cudaMemPrefetchAsync`来实现CPU和GPU之间的数据预取。

## CUDA加速应用程序的异步流和可视化分析

### 学习目标

* 使用Nsight Systems直观描述由GPU加速的CUDA应用程序的时间表
* 使用Nsight Systems识别和利用CUDA应用程序中的优化机会
* 利用CUDA流在被加速的应用程序中并发执行核函数

### 学习和实验

#### 比较异步预取

在上一章的学习过程中我们了解了异步预取对于程序性能的重要影响，即如果已经提前明确了在程序的哪一部分会发生内存的迁移，则可以提前将这部分数据预取到GPU或者CPU上，来减少分页错误。在本次实验中，就可以通过使用Nsight Systems这个可视化的性能分析工具，来观察异步预取对于程序的性能影响。

这里对比了不进行异步预取和提前进行异步预取这两种方式，在进行向量加法时的访存差异。可以看到，由于在进行向量加法之前，体检将数据预取到了GPU，因此在执行核函数的时候，并没有发生内存的迁移。

![image-20220615125537746](E:\project\parallel\Lab6\report\image-20220615125537746.png)

#### 并发CUDA流

在CUDA编程中，流时由按照顺序执行的一系列命令构成的。在CUDA应用程序中，核函数的执行以及一些内存传输均在CUDA流中进行。除了默认的流意外，CUDA还可以创建并使用非默认的CUDA流，此举可以支持执行多个操作。对于一个给定的流，其中的所有操作都会顺序执行，不同的非默认流之间是并行执行的，默认流拥有阻断能力，即他会等待所有其他的流执行完成后才开始执行自己的操作，并在此时阻塞其他的非默认流。

可以使用`cudaStreamCreate`来创建一个流，并且将得到的流作为一个参数传递给核函数，`someKernel<<<number_of_blocks, threads_per_block, 0, stream>>>()`。

![image-20220615131242726](E:\project\parallel\Lab6\report\image-20220615131242726.png)

## 证书

![image-20220615131738098](E:\project\parallel\Lab6\report\image-20220615131738098.png)