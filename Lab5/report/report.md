# 实验设计

考虑Gauss消去的整个过程中主要涉及到两个阶段，一个是在消元行行内除法过程，一个是其余行减去消元行的过程。而就每个阶段而言，其所做的工作基本是一致的，只是在不同的消元轮次时，消元的起始位置不同。尤其是针对第二个阶段，即其余行依次减去消元行的过程，这个阶段每一行所做的工作是完全一致的，十分适合并行化处理，即将待消去的行平均分配给几个不同的进程，由于这些数据之间不存在依赖性，因此每个进程只需要各自完成好自己的工作即可，不存在进程之间进行通信的额外开销，只需要在各个进程完成了分配的工作之后进行一次同步。

而当任务下发给不同的进程之后，在各个进程内又可以开启多条线程来处理任务，由于人物之间是不存在数据依赖的，因此不需要进行数据的通信，因此考虑在同一个进程内开启多条线程来处理任务是十分合理并且有效的。

而对于第一阶段，即消元行行内进行除法操作时，由于这个问题规模相对较小，如果将待操作的数据分配给不同的线程进行处理的话，线程挂起到唤醒这部分的时间开销相较于要处理的问题而言占比很高，因此不适合进行多线程并行处理，但是仍可以结合SIMD的向量化处理。同样在第二阶段，被消元行依次减去消元行的过程中，每一行内的减法运算同样也不适合进行多线程的并行处理，也可以采用SIMD进行向量化处理。

在本次实验中，将设计以下实验进行探究:

## MPI并行处理

对于Gauss消去的过程，在每一轮消去中主要包含两个阶段，首先是针对消元行做除法运算，然后是对于剩余的被消元行，依次减去消元行的某个倍数。在每一轮的过程中，除法操作和消元减法操作之间是有着严格的先后顺序的，即必须首先完成消元行的除法操作之后，才能够执行被消元行的减法操作。因此在除法运算和消元之间需要进行以及每一轮次消元结束之后需要进行一次同步。

在考虑使用MPI进行多进程的实验设计时，考虑使用单个进程首先来分发任务。然后在外层循环中，每个线程都要判断当前做除法的行是否是自己分配的任务，当这一行的除法操作完成之后，需要将除法的结果广播给其他所有进程，而其他进程接收除法结果的过程就自然的完成了除法后进程间的同步。

在这本次实验中，我们没有采用主从模式，即所有的进程都会去执行消元的任务，因此可以发现，消元的结果即使每次除法完成后的结果的累计，而除法做完之后都会进行一次广播，因此当最后一行完成之后，所有的除法结果都已经被广播到了各个进程中。因此可以省去最后一次的结果同步的过程。

可以计算出采用了MPI方式的优化算法，其理论加速比应该和进程数量成正比。但是由于进程之间的通信开销是十分大的，因此这种开销会大大降低优化效果。

## 数据划分对比

考虑到MPI属于进程级的并行手段，因此如果存在负载不均的情况，其浪费的资源要远远高于线程级的负载不均。因此在本次实验中也从负载均衡的角度出发，设计实验对比了不同数据划分方式的性能差异。主要对比了循环划分和块划分之间的差异。

最朴素的想法就是采用块划分的方式，即给各个进程分配连续的几行数据。但是考虑到Gauss消元的过程中，随着消元的进行，前面完成了除法的行将不会再参与后续的运算，也就是从整体的角度来考虑的话，各行之间的计算量是不同的，因此这种划分方式就会导致在后续的消元过程中，负责前面几行的进程处于闲置状态，因此是不能够接近理论加速比的。

而从负载均衡的角度出发，可以使用循环划分的方式。即将数据按照进程数量等步长分配给每个进程，这样从整体来看，每个进程负责的任务的计算量大致是相同的，不会导致严重的负载不均现象。因此在整个过程中，所有的进程基本保持满负荷，整体的加速比就会接近理论的加速比。

## 不同数据收发方式的实验对比

考虑到MPI是进程级的并行，因此消息通信是在不同进程之间完成的，而进程间的通信开销是远比线程级的通信开销要大的，因此合理的设计进程间的通信方式和限制通信次数对于提高性能优化会有比较明显的效果。

采用最朴素的方式就是当负责除法的进程完成了除法工作之后，将除法的结果依次发放给所有的进程，这个通信的开销是和进程的数量成正比的。而由于通信是需要时间的，因此当有些线程还没有接收到除法的结果的时候，是不能够开展后续的消元工作的。因此这里会存在着比较长的空闲等待。

而pipeline的方式就是利用流水线的思想去减少了这种空闲等待的性能损失，即当一个进程收到了除法的结果后，他需要将这个结果转发给下一个进程，之后就可以开始自己的工作了。这样就可以减少了负责除法工作进程的阻塞时间。

综合分析以上两种方法，采用广播的方式的时间开销为 $O(N^3logN)$，采用pipeline的方式的时间开销为 $O(N^3)$。

# 实验分析

## x86平台

### MPI并行实现及性能对比

为了能够探究MPI并行算法的优化效果，考虑调整问题规模，测量在不同任务规模下，串行算法，MPI优化，MPI+SIMD优化，MPI+OMP优化以及MPI+SIMD+OMP优化的时间性能表现。其中MPI采用了8个进程进行多进程并行，OMP采用了8条线程进行多线程并行，SIMD并采用了四路向量化处理。为了能够比较全面的展现并行优化效果随问题规模的变化情况，在问题规模小于1000时采用步长为100，而当问题规模大于1000时，步长调整为500。五种算法在不同问题规模下的表现如表所示。

为了能够更加直观的观察算法的性能表现随问题规模的变化情况，特意利用测量的数据计算了四种并行优化算法的加速比随时间的变化情况，如图所示。

从图像中可以看出，随着问题规模的增加，四种并行优化算法的加速比都呈现一个递增的趋势。就单独采用MPI进行多进程并行的优化方式而言，可以看到其随着任务规模的增加，加速比逐渐上升直至平稳在4.5左右，而实际开启了8条线程，其加速比并没有能够达到理论加速比，其原因在于通信开销以及其余没有进行多进程并行的部分。

在实验中还对比了基于MPI和SIMD、OMP的结合，并以MPI的性能为baseline，分析增加了其他优化方式的性能提升。

由于SIMD采用了4路向量化的手段，因此其相对于baseline的理论加速比应该能达到四倍。但是实验表明，SIMD实际的加速比只达到了2.2-2.5之间，这也和之前的实验向契合，证明在Gauss消元问题上，由于其他未能够进行SIMD向量化运算部分的影响，其理论加速比只能达到这个数值。

由于OMP拉起了8条线程，因此其相对于baseline的理论加速比应该能够达到8倍。但是实验表明，当问题规模达到3000的时候，这个性能提升也只达到了2.5倍左右，但是可以看出，随着问题规模的增加，这个加速比还有提升的趋势。分析原因，由于MPI多进程分配任务，每个进程上的任务只有总任务的$\frac{1}{8}$，因此，对于每个进程而言，即使问题规模达到了3000，分配给每个进程的任务也只有400左右，而通过之前的实验可以表明，在400左右的任务规模下，其加速比很难达到OpenMP的理论加速比。并且其趋势也能够表明，随着问题规模的增加，这个加速比应该是会继续上升的。

最后在实验中尝试了将MPI和SIMD、OpenMP融合到一起，同时启用多进程多线程向量化优化，在问题规模达到3000的时候，取得了将近20倍的性能提升。这和单独采用某一种优化方式取得的性能提升的乘积基本一致。

### 任务划分方式

在本次实验中，从负载均衡的角度触发，探究不同的任务划分方式对于算法性能的影响。由于MPI是进程级的并行方式，因此进程间的通信开销是很大的，所以要尽可能地利用每一个进程，并且减少进程之间的通信次数。实验对比了两种不同的任务划分方式，块划分和循环划分，实验结果如图所示。

从图像中可以看出，随着任务规模的增加，这两种划分方式的加速比都逐渐增加，但是随着任务规模的增加，循环划分的方式逐渐反超了块划分的方式。并且块划分方式率先达到了性能瓶颈，并且只达到了理论加速比的一半。这很符合预期，从整个过程来看，采用块划分的方式，随着消元的推进，负责前面行的进程会逐渐空闲下来，平均下来每个进程只有一半的时间在有效工作，因此其加速比只达到了4左右。而对于循环数据划分的方式而言，由于其从计算量的角度去划分任务，因此整体来看每个线程的有效工作时间几乎相同，达到了比较好的负载均衡，充分利用了每个线程的计算资源，因此其加速比逐步逼近理论加速比8。

### 数据收发方式对比

考虑到MPI在接收数据的时候处于阻塞状态，因此对于进行除法操作的进程而言，如果按照顺序向各个进程发送除法结果的话，那么该进程就需要等待所有的数据全部发送完成后才能开始处理自己的消元任务，而这个是完全浪费的时间。因此可以考虑借用流水线的思想，即从执行除法操作的进程开始，每个进程都只是接收上一个进程发来的除法结果，并将这个结果转发给下一个进程，然后就可以开始自己的消元任务。这样可以有效减少进程等待而浪费的时间。实验结果如图所示。

















