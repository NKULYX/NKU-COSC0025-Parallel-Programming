- [x] 代码实现
- [x] 代码优化：循环展开
- [ ] 性能测试
  - [x] 问题一
    - [x] 规模测试：不同大小规模下的时间
      - [x] 绘制走势，对比放一张图
      - [x] 找到趋势转折点，让cache大小不足
        - [x] 利用工具测试该点前后的命中率
  - [ ] 问题二
    - [ ] CPI对比
    - [x] 对比平台
      - [x] x86和arm Linux下

# 问题重述

## 体系结构相关实验分析——cache优化

计算给定n*n矩阵的每一列和给定向量的内积，考虑两种算法设计思路：

1. 逐列访问元素的平凡算法
2. cache的优化算法

## 体系结构相关实验分析——超标量优化

计算n个数的和，考虑两种算法的设计思路

1. 逐个累加的平凡算法
2. 超标量优化算法，如最简单的两路链式累加，或两两相加后中间结果再两两相加的递归算法

# 实验环境

## ARM架构

|   参数   |  数值  |
| :------: | :----: |
| CPU主频  | 2.6GHz |
| L1 cache |  64K   |
| L2 cache |  512K  |
| L3 cache |  48MB  |

## x86架构

|   参数   |         数值         |
| :------: | :------------------: |
| CPU型号  | Intel Core i7-11800H |
| CPU主频  |        2.3GHz        |
| L1 cache |         48K          |
| L2 cache |        1.25MB        |
| L3 cache |         24MB         |

# 实验设计及分析

## cache优化

针对给定的问题，由于矩阵在内存中存储时按照行有限的顺序存储的，也就是说，在内存中的矩阵是按行紧密排列的。因此，对于原始朴素的逐列访问算法来说，CPU会一次读入连续的一段数据到缓存中，其中可能只包含需要计算一个元素，因此当计算该列的第二个元素的时候，CPU又需要到更低的缓存或内存中去读取所需要的元素，而访存的时间相较于运算来说，开销是很大的，这会在很大程度上降低程序运行的效率。

因此我们考虑改进算法，采用逐列访问的cache优化算法，即充分利用每次读入的数据，将当前读入的缓存中的一行数据全部进行计算，然后累加到结果数组的对应位置，虽然在这个过程中并没有能够直接计算出结果，但是极大利用了cache中的缓存数据，减少去内存中寻找数据的访存时间。

同时，为了能够降低循环访问过程中，条件判断，指令跳转等额外开销，我们对于逐列访问的算法进行了进一步优化，采用循环展开的方法，在一次循环中，同时计算十个位置的值，可以利用多条流水线同时作业，发挥CPU超标量计算的性能。

为此，我们分别设计了三种算法，并在ARM架构的华为云鲲鹏服务器上进行测试

![p1](E:\学习资料\大二\并行程序设计\Experiment\Lab1\p1.jpg)

根据测试数据我们可以看出，在N<300的规模下，逐行或逐列的访问方式在效率上差别不大，但采用循环展开的优化算法能够取得较大的性能提升。而当300<N<3000时，逐列访问要比逐行访问的增长速度慢，但在此规模下，三种方法的增长趋势还是基本相同的。当N>3000之后，逐行访问的增长速度要明显超越了逐列访问的方法，证明此时cache优化起到了显著作用。

由数据可以进行理论分析，由于CPU的L1-L3的各级cache大小分别为64KB，512KB和48MB，对于数组元素为unsigned long long int而言，每一个元素占据10个字节，因此，填满各级cache的元素规模大概在80，200，2000。而图像的大概走势也恰恰能够印证这一假设，即在N<300下，尽管普通方法的L1 cache命中率可能已经很低，但是由于L2 cache的访问速度相对比较快，所以访存速度对整体的时间影响不太大，即两种逐行和逐列访问的方法效率差别不大。而当300<N<3000这个区间上时，L2 cache的命中率也会不断下降，使得逐行访问的方法被迫大更大的L3 cache中去寻找数据，这就导致了较大的访存开销，也可以看出两种方法的差距逐渐显现出来。而当数据规模超过3000之后，逐行访问的L3 cache命中率也会降到很低，也就是说逐行访问方法被迫到内存中去寻找数据，这导致的访存开销是非常大的，因此也使得两种方法的访问效率产生了显著的差异。

为了证明上述假设，我们分别采集了数据规模在300和4000的两组实验，通过VTune分析其各级缓存的访存次数的命中率。当N=80时，可以看到，逐行访问方法的L1 cache未命中率要显著高于逐列访问，导致其L2 cache的访问次数增多，但L2 cache大部分情况命中，所以两种方法效率差距不大。当N=300时，逐行访问L2 cache未命中率要显著高于逐列访问，导致其L3 cache的访问次数也显著高于逐列访问，但L3 cache绝大情况下都已命中。而L3 cache的访问时间相对开销较大，因此两种方法的时间差异开始显现出来。当N=4000时，我们可以看到，逐行访问方法的的L3 cache访问次数要远远高出逐列访问的方法，而且L3 cache的未命中率也高达27.1%，需要由大量的内存访问，时间开销会非常大，这也是两种方法产生巨大差距的主要原因。

|      | or        |          |          |         |         |        | op        |       |       |      |      |      |
| ---- | --------- | -------- | -------- | ------- | ------- | ------ | --------- | ----- | ----- | ---- | ---- | ---- |
| n    | L1        |          | l2       |         | l3      |        | l1        |       | l2    |      | l3   |      |
| 80   | 78113853  | 318810   | 310911   | 7908    | 264     | 12     | 91436268  | 4082  | 3858  | 137  | 120  | 0    |
| 300  | 145250802 | 13056978 | 12844359 | 212616  | 162792  | 150    | 183033393 | 56220 | 51114 | 5121 | 936  | 0    |
| 4000 | 149440947 | 15846024 | 11914602 | 3931422 | 2389212 | 888993 | 145316769 | 80232 | 76473 | 3762 | 357  | 87   |

对于循环展开方法对逐列访问方式的优化，由于循环展开可以在一个循环周期内利用多条流水线并行执行相同指令，因此能够在一定程度上优化代码运行效率。这一点通过比较两种方法的CPI也能够得到印证。

| optimize | optimize+unroll |
| :------: | :-------------: |
|  0.4975  |     0.4761      |

## 超标量优化

对于给定的问题，要求计算N个数的和，对于常规的顺序算法而言， 由于每次都是在同一个累加变量上进行累加，导致只能调用CPU的一条流水线进行处理，无法充分发挥CPU超标量优化的性能，因此考虑使用多链路的方法对传统的链式累加方法进行改进，即设置多个临时变量，在一个循环内同时用着多个临时变量对多个不同的位置进行累加，达到多个位置并行累加的效果，同时还能够减少循环遍历的步长，降低循环开销。由于多链路方法使用了循环展开技术在一定程度上降低了循环的额外开销，为了保证实验的准确性，我们对普通的链式累加方法也要进行同样比例的循环展开，控制实验的可变因素，使得实验结果具有合理的对比性。

通过对比在不同实验规模下的两种方法的运行时间，探究优化加速比同问题规模的变化情况，并分析其中的内在原因。此外，还将会探究在x86架构下，Windows和Linux两种系统对于处理同样规模的问题所需要消耗的绝对时间，以及优化的加速比的情况。

为了方便算法的实现，我们的所有问题规模都取成2的n次幂，由于当问题规模较小的时候，两种算法并没有显著的时间效率差异，因此我们直接扩大了问题规模，分别测试了从n=9到28之间的20组数据，如表所示。

由表中的数据可以看出，无论是链式累加的方法还是多链路展开的方法，由于都属于线性时间效率的方法，因此随着问题规模的翻倍，时间也近似翻倍。采用双链路展开的超标量优化方法的时间效率要显著高于普通的链式累加方法，这是因为，通过多链路的方法，将相互联系的累加解耦成了两路不相关的问题，使得CPU能够同时调用两条流水线处理问题，实现超标量优化的目的。为了进一步探究超标量优化的加速比，我们对这二十组数据计算了优化加速比同问题规模的变化情况，如图所示。

<img src="E:\学习资料\大二\并行程序设计\Experiment\Lab1\p2.jpg" alt="p2" style="zoom:50%;" />

通过实验数据可以发现，当问题规模较小的时候，算法的优化加速比比较低，但随着问题规模的增大，算法的优化加速比呈现先增加再保持最后降低的趋势。其增加的原因主要是由于多链路方法将累加拆分成了两个不相关的部分，能够利用CPU的超标量优化，使得算法获得逐渐增高的加速比。当问题问题规模$2^{10}$到$2^{17}$之间的时候，优化加速比基本保持一个稳定的状态，说明此时的超标量优化已经达到一个上限。而当问题规模超过$2^{17}$后，整体呈现一个下降的趋势，猜测是由于问题规模过大，导致缓存不足，由于需要经常进行内外存的访问，导致了较大的访存开销，而这个访存开销占据了程序运行的大部分时间，所以超标量优化的效果被一定程度上减弱。为了验证上述想法，我们利用VTune对下降最显著的点分析各级缓存的访问和命中情况，选取了$2^{22}$这个点分析，如表所示。

通过数据可以得到，当问题规模在$2^{12}$时，所有的问题几乎全部在L1 cache命中，L2 cache和L3 cache在过程中几乎没有访问，而且命中率也几乎在100%。而当问题规模达到$2^{22}$时，虽然L1 cache的命中率还是接近100%，但是已经有了很多的L2 cache和L3 cache访问，而且L3 cache的命中率只有80%，会出现很多的内存访问，这将会极大的影响程序运行的时间，因此印证了我们上面的猜测。
