# 问题描述

在进行科学计算的过程中，经常会遇到对于多元线性方程组的求解，而求解线性方程组的一种常用方法就是Gauss消去法。即通过一种自上而下，逐行消去的方法，将线性方程组的系数矩阵消去为主对角线元素均为1的上三角矩阵。
$$
\left[
\begin{matrix}
a_{11}&a_{12}&\cdots&a_{1\ n-1}&a_{1n}\\
a_{21}&a_{22}&\cdots&a_{2\ n-1}&a_{2n}\\
\vdots&\vdots&\ddots&\vdots&\vdots\\
a_{n-1\ 1}&a_{n-1\ 2}&\cdots&a_{n-1 \ n-1}&a_{n-1\ n}\\
a_{n\ 1}&a_{n\ 2}&\cdots&a_{n \ n-1}&a_{n\ n}\\
\end{matrix}
\right]
=>
\left[
\begin{matrix}
1&a_{12}'&\cdots&a_{1\ n-1}'&a_{1n}'\\
0&1&\cdots&a_{2\ n-1}'&a_{2n}'\\
\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&\cdots&1&a_{n-1\ n}\\
0&0&\cdots&0&1\\
\end{matrix}
\right]
$$
在整个消去的过程中，主要包含两个过程

![image-20220428214131431](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220428214131431.png)

# 实验设计

考虑Gauss消去的整个过程中主要涉及到两个阶段，一个是在消元行行内除法过程，一个是其余行减去消元行的过程。而就每个阶段而言，其所做的工作基本是一致的，只是在不同的消元轮次时，消元的起始位置不同。尤其是针对第二个阶段，即其余行依次减去消元行的过程，这个阶段每一行所做的工作是完全一致的，十分适合并行化处理，即将待消去的行平均分配给几个线程，由于这些数据之间不存在依赖性，因此每个线程只需要各自完成好自己的工作即可，不存在线程之间进行通信的额外开销。

而对于第一阶段，即消元行行内进行除法操作时，由于这个问题规模相对较小，如果将待操作的数据分配给不同的线程进行处理的话，线程挂起到唤醒这部分的时间开销相较于要处理的问题而言占比很高，因此不适合进行多线程并行处理，但是仍可以结合SIMD的向量化处理。同样在第二阶段，被消元行依次减去消元行的过程中，每一行内的减法运算同样也不适合进行多线程的并行处理，也可以采用SIMD进行向量化处理。

在本次实验中，将设计以下实验进行探究：

* 采用openmp对于Gauss消去的算法进行多线程优化，分析在不同任务规模下的性能表现
* 设计不同的任务划分方式，调整schedule的参数，对比在不同任务规模下的性能表现
* 使用openmp进行SIMD向量化处理，并与之前手动SIMD的性能进行对比
* 使用openmp进行多线程处理，并与之前手动pthread的性能进行对比
* 对比不同的任务划分方式之间的性能差异，主要对比按行划分和按列划分
* 对比在openmp下采用不同的线程创建方式的性能差异，主要对比动态线程创建和静态线程创建
* 对比在不同线程数量下的openmp的性能表现情况
* 尝试将openmp优化方法从arm平台迁移到x86平台上，分析性能表现
* 在devcloud上尝试任务卸载，将Gauss消元的消去过程卸载到GPU上进行运算，测量其性能表现

## openmp并行处理

对于Gauss消去的过程，在每一轮消去中主要包含两个阶段，首先是针对消元行做除法运算，然后是对于剩余的被消元行，依次减去消元行的某个倍数。在每一轮的过程中，除法操作和消元减法操作之间是有着严格的先后顺序的，即必须首先完成消元行的除法操作之后，才能够执行被消元行的减法操作。因此在除法运算和消元之间需要进行以及每一轮次消元结束之后需要进行一次同步。

在考虑使用openmp进行多线程的实验设计时，考虑使用单个线程首先来处理消元行的除法操作。当这一行的除法操作完成之后，再将后续的消元过程分配给多个线程，并将循环变量全部声明为线程私有，将待消元的矩阵声明为线程间共享。

由于openmp还提供的simd的自动向量化预编译选项，因此在实验探究的过程中，还将探究单线程条件下使用了simd预编译选项后的性能表现，并且同之前手动设计的SIMD向量化算法的性能表现进行对比。此外还会将openmp设计的多线程程序和之前设计的pthread多线程程序进行性能对比，对比两种不同的算法设计的性能差异。

可以计算出采用了openmp方式的优化算法，其理论加速比应该和线程数量成正比，最优效果下应该能够达到 NUM_THREADS 倍的性能提升。而当同时结合了 SIMD 算法之后，这个加速比又将提升，并于向量化处理的宽度有关。当采用四路向量化时，整体的最优加速比应该能够达到 $NUM_THREADS \times 4 $ 倍。

## 数据划分设计

openmp为我们提供了多种的任务划分方式，可以通过设置schedule中的参数选择不同的任务划分方式。针对不同的负载特性，可以考虑使用static、dynamic和guided这三种不同的任务划分方式。

最朴素的想法就是采用静态数据划分的方式，即在给各个线程分配任务的时候就已经确定好了每个线程负责的任务范围。这种任务划分方式，在有些情况下，即任务分布不均匀的时候会导致比较严重的负载不均。而在Gauss消去的过程中，由于每个阶段都是重新进行任务划分的，因此负载不均的问题并不明显，因而直接采用静态数据划分的方式也应该能够收获不错的效果。

而从负载均衡的角度出发，可以使用动态数据划分的方式。由于在每一轮消元的过程中，最终决定本轮消元时间的是运行时间最长的线程。因此可能存在个别线程提前完成任务而进入空闲等待浪费了计算资源。因此可以使用动态任务划分的方式，但可能会导致较大的额外线程调度开销。

由于高斯消元的过程中，任务的规模在不断减小，因此如果划分给每个线程的任务范围是固定的，就会导致随着任务的逐步推进，个别线程在某一轮消元的过程中没有被分配到任何任务，而其他线程则需要执行很多的任务，因此浪费了比较多的计算资源。因此可以使用schedule中提供的guided参数，随着任务推进，指数缩减到指定的步幅，这样就可以尽可能地利用上全部线程地计算资源。

## 行列划分设计

考虑不同的任务划分方式，对于Gauss消元的问题，可以考虑从行列两种划分角度进行实验设计。通常的，我们都是采用按行划分的方式，原因在于，在高斯消元的过程中，每一行的所需要做的操作是完全相同的，并且行内的数据是连续的，因此具有比较好的空间局部性。但是由于外层循环的迭代，每一轮各个线程所处理的行号可能是完全不同的，这将会导致在不同的消元轮次中，各个线程的L1和L2cache中不存在所需要的数据，而需要到其他线程的cache中去访问，这就造成了伪共享。而线程之间访问cache也是一个很大的开销，因此按行划分的方式会受到时间局部性的制约。

考虑按列划分的方式。在进行高斯消元的过程中，每一列所做的操作也是完全相同的，即将消元行该列的值依次减到该列中的其余位置处，而各列之间又不存在着数据依赖。因此也可以考虑按列划分的任务方式。但是由于在计算的过程中，每一个线程中的数据访问是跨行的，因此可能存在着比较大的几率导致cache miss，因此这种方法受到空间局部性的制约。而由于Gauss消元的过程是逐渐向右下角收缩的过程，因此按列划分的方式也会受到时间局部性的制约。但是按列划分也是一种合理的尝试。

## 不同数据规模和线程数下的性能探究

考虑到线程的创建，调度，挂起和唤醒等操作相对于简单的计算操作而言，所需要的时间开销是非常大的。因此可以推测，当问题规模比较小的时候，由于线程调度导致的额外开销会抵消掉多线程优化效果，甚至还会表现出多线程比串行算法更慢的情况。而随着问题规模的增加，线程之间调度切换所需要的时间开销相对于线程完成任务所需要的时间而言已经占比很低，这样就能够正常反映出多线程并行优化的效果。因此，设计实验探究在不同数据规模下，多线程并行优化算法的优化效果。此外还将探究在所使用的线程数量不同的情况下，并行算法优化效果的变化情况。

## x86平台迁移

本次实验除了对ARM架构下采用neon指令集架构结合openmp多线程编程，对Gauss消去算法进行并行化处理，还将算法迁移到了x86平台上，采用x86中的SSE指令集架构分别对算法进行重构，然后对比实验效果。

## 任务卸载实验设计

Intel的在线平台devcloud为实验提供了很好的平台，能够在在线平台上尝试使用GPU进行运算卸载。因此可以考虑将数据以来较小，运算密集的任务卸载到GPU上进行加速运算。

针对Gauss消元的问题，可以发现，对于每一轮循环，可以将

# 实验结果分析

## ARM平台

### pthread并行处理

为了能够探究pthread并行算法的优化效果，考虑调整问题规模，测量在不同任务规模下，pthread并行优化算法对于普通串行算法和SIMD向量化优化算法的加速比。在本次实验中，pthread并行算法中，同样融合了SIMD的向量化处理。在ARM平台上，SIMD的实现是基于Neon指令集架构的。为了能够比较全面的展现并行优化效果随问题规模的变化情况，在问题规模小于1000时采用步长为100，而当问题规模大于1000时，步长调整为1000。三种算法的在不同问题规模下的表现如下表所示。

在实验设计时，SIMD进行向量化处理的时候，采用的是四路向量化处理，而pthread多线程优化时，总共开启了8条线程，其中一条线程负责除法操作，剩余的7条线程负责做消元操作。因此从时间表现情况来看，理论上SIMD优化算法所需要的时间应该是串行算法的$\frac{1}{4}$，pthread多线程所需要的时间应该是SIMD向量化的$\frac{1}{7}$。而从实验数据来看，当问题规模较小的时候，pthrad多线程算法的时间性能甚至差于普通的串行算法。这是由于线程的创建，挂起，唤醒和切换等操作，所需要消耗的时钟周期数要远远多余简单的运算操作。因此当问题规模较小时，由于运算操作在整个问题求解的过程中所占比例较低，因此线程额外开销的副作用就会显现出来。而随着问题规模的增大，pthread多线程的优势就能够显现出来。两种并行优化算法的加速比变化如下图所示。

从图像中可以看出，SIMD的加速比随着问题规模的增加基本保持稳定，由于算法中还涉及到其他的数据处理，因此其加速比只达到了2左右，并没有能够达到理论上的4。而pthread优化的效果则随着问题规模的增加呈现出持续上升的趋势。这是因为，问题规模的增加，使得程序在运行的过程中，运算所占比例不断上升，这将会逐步抵消由于线程切换导致的额外开销。从数据中可以看出，当问题规模达到2000时，已经接近了其对SIMD的理论加速比。可以推测，当问题规模持续上升时，这个加速比将会接近7。

### 数据划分方式对比

本次实验中，除了进行基础的pthread多线程优化尝试之外，还从数据划分的角度出发，考虑不同的数据划分方式，对于并行算法优化效果的影响。结合前文实验设计，分别对比了循环划分，块划分和动态划分三种方式，在不同问题规模下的表现效果，并以SIMD算法为baseline，其加速比变化情况如下图所示。

可以看到，随着问题规模的增大，这三种任务划分方式的加速比都是逐渐去接近理论加速比的。但是也可以注意到，在三种任务划分之间的性能表现还是存在着明显差异的。

从cache优化的角度出发，循环划分和块划分的主要区别就在于能都利用到cache优化。就循环划分这种方式而言，线程在处理完当前行之后，接下来要处理的行距离当前间隔为NUM_PHTREAD，因此当数据规模很大的时候，会因为L1 cache不能够容纳下足够的数据，或者由于CPU未能够及时的预取下一行数据，而导致cache miss，因此需要额外的访存开销。而块划分的方式就能够很好的弥补这一点，其原因是对于每个线程而言，他所需要处理的数据之间在内存上是连续的，因此有很好的cache优势，因此能够减小由于cache miss导致的额外访存开销。使用perf工具对于这两种算法的L1 cache的命中率进行检测，如下表所示。块划分的命中率能够达到98%，而循环划分的方式只有94%左右，两者差异不大，因此在性能表现上的差异也不显著。

从负载均衡的角度出发，循环划分和动态数据划分的主要区别就在于能否充分利用各个线程的计算资源，尽可能减少同步等待所导致的额外开销。如果采用循环数据划分的方式，由于各个线程完成任务所需要的时间不尽相同，并且由于问题规模可能不是线程的整数倍，因此可能存在某些线程较早完成任务进入同步等待状态，而其他线程还未完成任务，因此就浪费了一些计算资源。而动态数据划分就是从这个角度出发，尽可能充分利用每个线程的计算资源，使得任务能够在线程之间得到比较均匀的划分。从图中也可以看出，当问题规模较小的时候，动态划分方式的表现不如循环划分，这是由于动态划分在保证负载均衡的前提下，牺牲了线程调度的开销，由于每个线程不清楚自己具体的工作，因此会存在比较大的线程同步和线程切换的开销，这种额外开销在问题规模比较小的时候会格外显著。而当问题规模提升的时候，可以发现，动态划分方式的表现已经能够超越循环划分，负载均衡带来的收益已经抵消了线程调度的额外开销。

### 线程数量对比

本次实验中，还探究了pthread多线程优化方法，在开启不同的线程数量时，优化效果的变化情况。为了能够显著体现pthread的优化效果，选取数据规模为1000，调整线程数量，观测加速比的变化情况如下图所示。

从图像中可以看出，随着线程数量的线性增加，pthread多线程的优化效果也是呈现出线性提升的趋势。而当线程数量超过8个之后，其优化效果不再有显著的变化。这是由于实验使用的服务器单CPU核心能够提供8个线程，因此当线程数量小于8个的时候，CPU核心能够使用自己的8个线程调度任务，而当所需要的线程数量超过8个之后，就需要和服务器中的其他CPU核心借用线程，这之间会存在着额外的调度开销，因此抵消掉了性能的提升效果。

## x86平台迁移

### 多种SIMD指令集融合

基于前文在ARM平台上对于pthread多线程编程的探究，在本次实验中还将pthread多线程优化方法迁移到x86平台上，做同样的实验探究。由于x86平台上拥有更多的SIMD指令集架构，因此实验中分别探究了SSE、AVX和AVX512三种指令集架构配合pthread多线程的优化效果，测量在不同问题规模下的运行时间，如下表所示。可以看出，pthread多线程可以结合多种SIMD指令集架构，并且在各种指令集架构上的表现基本保持稳定，并没有出现在某种指令集架构下不能够发挥很好的多线程优势的现象。

此外，实验还以SSE指令集架构为例，探究了随着问题规模的变化，不同SSE向量化处理和pthread多线程结合SSE向量化处理这两种方法的表现情况，变化趋势图如下图所示。

可以看出，在问题规模小于1000的时候，加速比随着问题规模的线性增长呈现出一个线性上升的趋势。而当问题规模超过1000的时候，会发现加速比出现了一个下降的趋势。结合VTune性能分析工具分析的结果，分析其原因是因为，当问题规模增加时，超过了线程cache的大小，导致出现了大量的cache miss，额外的访存开销在一定程度上抵消了多线程的优化效果，使得加速比的变化出现拐点。

### 不同任务划分方式对比

从cache优化的角度出发，循环划分和块划分的主要区别就在于能都利用到cache优化。就循环划分这种方式而言，线程在处理完当前行之后，接下来要处理的行距离当前间隔为NUM\_PHTREAD，因此当数据规模很大的时候，会因为cache不能够容纳下足够的数据，或者由于CPU未能够及时的预取下一行数据，而导致cache miss，因此需要额外的访存开销。而块划分的方式就能够很好的弥补这一点，其原因是对于每个线程而言，他所需要处理的数据之间在内存上是连续的，因此有很好的cache优势，因此能够减小由于cache miss导致的额外访存开销。使用VTune工具对于这两种算法的L1 cache的命中率进行检测，如下表所示。块划分的命中率能够达到98\%，而循环划分的方式只有92\%左右，因此，对于块划分而言，由于其考虑到了cache特性，因此随着问题规模的增大，其性能并未明显受到访存开销的影响。而对于循环数据划分，则因为其划分方式会导致大量的cache miss，因此访存开销会极大影响其性能表现。这也正符合图中的变化趋势。

从负载均衡的角度出发，循环划分和动态数据划分的主要区别就在于能否充分利用各个线程的计算资源，尽可能减少同步等待所导致的额外开销。从图\ref{fig:ff6}中也可以看出，当问题规模较小的时候，动态划分方式的表现不如循环划分，这是由于动态划分在保证负载均衡的前提下，牺牲了线程调度的开销，由于每个线程不清楚自己具体的工作，因此会存在比较大的线程同步和线程切换的开销，这种额外开销在问题规模比较小的时候会格外显著。而当问题规模提升的时候，可以发现，动态划分方式的表现已经能够超越循环划分，负载均衡带来的收益已经抵消了线程调度的额外开销。根据VTune性能分析工具，观察这三种任务划分方式的CPU占用率，可以得到如下对比图。从途中可以看出，当动态数据划分的CPU占用率一直保持一个较高水平，并且相对比较均衡。而对比其余两种划分方式，由于其没有考虑负载均衡，因此在CPU占用率这个指标上，其波动十分明显，甚至会出现低于20%的占用率，这是对于计算资源的严重浪费。