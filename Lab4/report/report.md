# 问题描述

在进行科学计算的过程中，经常会遇到对于多元线性方程组的求解，而求解线性方程组的一种常用方法就是Gauss消去法。即通过一种自上而下，逐行消去的方法，将线性方程组的系数矩阵消去为主对角线元素均为1的上三角矩阵。
$$
\left[
\begin{matrix}
a_{11}&a_{12}&\cdots&a_{1\ n-1}&a_{1n}\\
a_{21}&a_{22}&\cdots&a_{2\ n-1}&a_{2n}\\
\vdots&\vdots&\ddots&\vdots&\vdots\\
a_{n-1\ 1}&a_{n-1\ 2}&\cdots&a_{n-1 \ n-1}&a_{n-1\ n}\\
a_{n\ 1}&a_{n\ 2}&\cdots&a_{n \ n-1}&a_{n\ n}\\
\end{matrix}
\right]
=>
\left[
\begin{matrix}
1&a_{12}'&\cdots&a_{1\ n-1}'&a_{1n}'\\
0&1&\cdots&a_{2\ n-1}'&a_{2n}'\\
\vdots&\vdots&\ddots&\vdots&\vdots\\
0&0&\cdots&1&a_{n-1\ n}\\
0&0&\cdots&0&1\\
\end{matrix}
\right]
$$
在整个消去的过程中，主要包含两个过程

![image-20220428214131431](C:\Users\Lenovo\AppData\Roaming\Typora\typora-user-images\image-20220428214131431.png)

# 实验设计

考虑Gauss消去的整个过程中主要涉及到两个阶段，一个是在消元行行内除法过程，一个是其余行减去消元行的过程。而就每个阶段而言，其所做的工作基本是一致的，只是在不同的消元轮次时，消元的起始位置不同。尤其是针对第二个阶段，即其余行依次减去消元行的过程，这个阶段每一行所做的工作是完全一致的，十分适合并行化处理，即将待消去的行平均分配给几个线程，由于这些数据之间不存在依赖性，因此每个线程只需要各自完成好自己的工作即可，不存在线程之间进行通信的额外开销。

而对于第一阶段，即消元行行内进行除法操作时，由于这个问题规模相对较小，如果将待操作的数据分配给不同的线程进行处理的话，线程挂起到唤醒这部分的时间开销相较于要处理的问题而言占比很高，因此不适合进行多线程并行处理，但是仍可以结合SIMD的向量化处理。同样在第二阶段，被消元行依次减去消元行的过程中，每一行内的减法运算同样也不适合进行多线程的并行处理，也可以采用SIMD进行向量化处理。

在本次实验中，将设计以下实验进行探究：

* 采用openmp对于Gauss消去的算法进行多线程优化，分析在不同任务规模下的性能表现
* 设计不同的任务划分方式，调整schedule的参数，对比在不同任务规模下的性能表现
* 使用openmp进行SIMD向量化处理，并与之前手动SIMD的性能进行对比
* 使用openmp进行多线程处理，并与之前手动pthread的性能进行对比
* 对比不同的任务划分方式之间的性能差异，主要对比按行划分和按列划分
* 对比在openmp下采用不同的线程创建方式的性能差异，主要对比动态线程创建和静态线程创建
* 对比在不同线程数量下的openmp的性能表现情况
* 尝试将openmp优化方法从arm平台迁移到x86平台上，分析性能表现
* 在devcloud上尝试任务卸载，将Gauss消元的消去过程卸载到GPU上进行运算，测量其性能表现

## openmp并行处理

对于Gauss消去的过程，在每一轮消去中主要包含两个阶段，首先是针对消元行做除法运算，然后是对于剩余的被消元行，依次减去消元行的某个倍数。在每一轮的过程中，除法操作和消元减法操作之间是有着严格的先后顺序的，即必须首先完成消元行的除法操作之后，才能够执行被消元行的减法操作。因此在除法运算和消元之间需要进行以及每一轮次消元结束之后需要进行一次同步。

在考虑使用openmp进行多线程的实验设计时，考虑使用单个线程首先来处理消元行的除法操作。当这一行的除法操作完成之后，再将后续的消元过程分配给多个线程，并将循环变量全部声明为线程私有，将待消元的矩阵声明为线程间共享。

由于openmp还提供的simd的自动向量化预编译选项，因此在实验探究的过程中，还将探究单线程条件下使用了simd预编译选项后的性能表现，并且同之前手动设计的SIMD向量化算法的性能表现进行对比。此外还会将openmp设计的多线程程序和之前设计的pthread多线程程序进行性能对比，对比两种不同的算法设计的性能差异。

可以计算出采用了openmp方式的优化算法，其理论加速比应该和线程数量成正比，最优效果下应该能够达到 NUM_THREADS 倍的性能提升。而当同时结合了 SIMD 算法之后，这个加速比又将提升，并于向量化处理的宽度有关。当采用四路向量化时，整体的最优加速比应该能够达到 $NUM_THREADS \times 4 $ 倍。

## 数据划分设计

openmp为我们提供了多种的任务划分方式，可以通过设置schedule中的参数选择不同的任务划分方式。针对不同的负载特性，可以考虑使用static、dynamic和guided这三种不同的任务划分方式。

最朴素的想法就是采用静态数据划分的方式，即在给各个线程分配任务的时候就已经确定好了每个线程负责的任务范围。这种任务划分方式，在有些情况下，即任务分布不均匀的时候会导致比较严重的负载不均。而在Gauss消去的过程中，由于每个阶段都是重新进行任务划分的，因此负载不均的问题并不明显，因而直接采用静态数据划分的方式也应该能够收获不错的效果。

而从负载均衡的角度出发，可以使用动态数据划分的方式。由于在每一轮消元的过程中，最终决定本轮消元时间的是运行时间最长的线程。因此可能存在个别线程提前完成任务而进入空闲等待浪费了计算资源。因此可以使用动态任务划分的方式，但可能会导致较大的额外线程调度开销。

由于高斯消元的过程中，任务的规模在不断减小，因此如果划分给每个线程的任务范围是固定的，就会导致随着任务的逐步推进，个别线程在某一轮消元的过程中没有被分配到任何任务，而其他线程则需要执行很多的任务，因此浪费了比较多的计算资源。因此可以使用schedule中提供的guided参数，随着任务推进，指数缩减到指定的步幅，这样就可以尽可能地利用上全部线程地计算资源。

## 行列划分设计

考虑不同的任务划分方式，对于Gauss消元的问题，可以考虑从行列两种划分角度进行实验设计。通常的，我们都是采用按行划分的方式，原因在于，在高斯消元的过程中，每一行的所需要做的操作是完全相同的，并且行内的数据是连续的，因此具有比较好的空间局部性。但是由于外层循环的迭代，每一轮各个线程所处理的行号可能是完全不同的，这将会导致在不同的消元轮次中，各个线程的L1和L2cache中不存在所需要的数据，而需要到其他线程的cache中去访问，这就造成了伪共享。而线程之间访问cache也是一个很大的开销，因此按行划分的方式会受到时间局部性的制约。

考虑按列划分的方式。在进行高斯消元的过程中，每一列所做的操作也是完全相同的，即将消元行该列的值依次减到该列中的其余位置处，而各列之间又不存在着数据依赖。因此也可以考虑按列划分的任务方式。但是由于在计算的过程中，每一个线程中的数据访问是跨行的，因此可能存在着比较大的几率导致cache miss，因此这种方法受到空间局部性的制约。而由于Gauss消元的过程是逐渐向右下角收缩的过程，因此按列划分的方式也会受到时间局部性的制约。但是按列划分也是一种合理的尝试。

## 不同数据规模和线程数下的性能探究

考虑到线程的创建，调度，挂起和唤醒等操作相对于简单的计算操作而言，所需要的时间开销是非常大的。因此可以推测，当问题规模比较小的时候，由于线程调度导致的额外开销会抵消掉多线程优化效果，甚至还会表现出多线程比串行算法更慢的情况。而随着问题规模的增加，线程之间调度切换所需要的时间开销相对于线程完成任务所需要的时间而言已经占比很低，这样就能够正常反映出多线程并行优化的效果。因此，设计实验探究在不同数据规模下，多线程并行优化算法的优化效果。此外还将探究在所使用的线程数量不同的情况下，并行算法优化效果的变化情况。

## x86平台迁移

本次实验除了对ARM架构下采用neon指令集架构结合openmp多线程编程，对Gauss消去算法进行并行化处理，还将算法迁移到了x86平台上，采用x86中的SSE指令集架构分别对算法进行重构，然后对比实验效果。

## 任务卸载实验设计

Intel的在线平台devcloud为实验提供了很好的平台，能够在在线平台上尝试使用GPU进行运算卸载。因此可以考虑将数据以来较小，运算密集的任务卸载到GPU上进行加速运算。

针对Gauss消元的问题，可以发现，对于每一轮循环，可以将消元的过程卸载到GPU中进行运算，针对每个GPU计算单元，需要将矩阵作为共享数据分发到每个GPU运算单元的内存中，而将其余的循环控制变量作为私有变量，防止不同的GPU运算单元之间相互影响。

因此，在设计实验的时候，在每轮循环中，首先让主线程去处理除法操作，然后将隔行的消元操作卸载到GPU上。对于卸载到GPU中的运算部分，可以使用SIMD进行向量化的处理，充分利用GPU中的矩阵计算优势，以期望能够获得很好的性能表现。

# 实验结果分析

## ARM平台

### openmp实现及性能对比

为了能够探究openmp并行算法的优化效果，考虑调整问题规模，测量在不同任务规模下，串行算法，手动SIMD算法，手动pthread算法以及openmp版本的SIMD算法、openmp版本的多线程算法的时间性能表现。其中pthread多线程算法和openmp多线程算法都和SIMD算法进行了结合，启用了8条线程，并采用了四路向量化处理。为了能够比较全面的展现并行优化效果随问题规模的变化情况，在问题规模小于1000时采用步长为100，而当问题规模大于1000时，步长调整为500。五种算法在不同问题规模下的表现如下表所示。

为了能够更加直观的观察算法的性能表现随问题规模的变化情况，特意利用测量的数据计算了四种并行优化算法的加速比随时间的变化情况，如图所示。

从图像中可以看出，随着问题规模的增加，四种并行优化算法的加速比都呈现一个递增的趋势。由于在计算的过程中，出了乘法操作和消元操作，还有很多和分支跳转等不能够进行向量化处理的运算，因此采用SIMD思路进行优化的两种方法并没有能够接近理论加速比4倍，之保持在了一个1.5左右的加速比。而对于两种多线程的并行优化算法，其加速比随着问题规模的增加呈现快速增长的趋势，并且由于两种多线程的算法都与SIMD算法进行了融合，因此在评价其性能的时候可以用SIMD的性能作为baseline。以此为参考时，可以发现，随着问题规模的增加，两种并行优化算法都接近了对SIMD的理论加速比8倍，这说明Gauss消元的问题是十分适合进行多线程优化的，原因是因为不同的线程之间在分配任务的时候并不存在严重的数据以来问题，因此线程之间额外的通信开销很少。

此外，实验还对比了手动的SIMD算法和openmp版本的SIMD算法之间的性能差异，从图像中可以看出，手动的SIMD算法要略优于openmp版本，通过perf来分析其性能可以发现，openmp版本的所需要的指令数instructions要明显高于手动SIMD的方法，并且在时钟周期cycles指标上也要高于手动版本，因此其性能表现更差。

同时，实验还对比了手动的pthread多线程和openmp多线程之间的性能差异，从图像中可以看出，手动的pthread算法其性能表现要一直差于openmp，这是因为在手动实现pthread多线程算法的时候，由于Gauss消元的各个轮次之间存在着严格的先后关系，每一轮内部的除法操作和消元操作也都存在着严格的依赖关系，因此在手动实现多线程的时候，线程的同步开销要比openmp管理所需要的时间更多。

### 数据划分对比

openmp在进行数据划分schedule的时候，为我们提供了三种不同的选项，即static、dynamic和guided。static方法是在线程创建完成之后，就明确划分了任务，dynamic方法则是在线程执行的过程中去动态的划分任务，而guided方法则是随着任务的推进逐步缩减任务划分的粒度。在本次实验中，我们对比了这三种方法在不同任务规模下的性能表现，并将绘制出了加速比随问题规模的变化情况，如图所示。

从图像中可以看出，随着任务规模的增加，这三种划分方式的加速比都逐渐增加，而static方法首先达到了瓶颈，这是因为10-12左右就是openmp的一个最大理论加速比的范围，因此在Gauss消元这个问题上static的性能表现是最好的。原因是，在Gauss消元的过程中，每一轮都会重新进行任务划分，因此虽然整体上来看，矩阵的每个部分的计算量是不同的，但是在每一个消元轮次内，任务基本上是均匀的，因此不存在严重的负载不均的问题。而static和guided主要是为了解决这种负载不均的现象，为了平衡负载不均，必定要采用更细粒度的任务划分，因此会涉及到更多的额外条件判断以及通信开销，因此在Gauss消元问题上的表现不如static方法。

同样是考虑了负载均衡，由于guided方法会逐步缩减任务划分的粒度，尽可能让所有线程都被分配任务，而动态划分的方式可能出现随着任务推进，个别线程不会被分配到任务的情况，因此浪费了一定的计算资源。所以从图像中也可以看出，采用guided逐步调整划分粒度的方法其性能表现要比dynamic方法更优。

### 行列划分对比

在本次实验中，除了从负载均衡的角度出发，探究不同的粒度的任务划分方式以外，还从cache的角度出发，考虑空间局部性和时间局部性，探究了行列划分两种方式的性能表现，其性能表现如图所示。

从图像中可以很明显的看到，由于按列计算的方式，对于矩阵的访问来说是跨行访问的，因此随着任务规模的增加，由于空间局部性的限制，会产生比较严重的cache miss，因此在访存的时候需要到内存中或者其他线程的cache中去访问，这会导致很严重的访存开销。通过perf分析两种方式的L1 dcache miss情况，如图所示，这也能够很好的印证我们的推断。

而从时间局部性考虑，这两种方法都会受到时间局部性的制约。这是由于随着任务的推进，任务执行的区域逐渐缩减，因此对于每个线程而言，其被分配的任务行号或者列号可能每次都不一样，因此就不能够利用上一轮计算中已经缓存到cache中的数据。因此可以从这个角度出发进一步改良算法。

### 线程数量对比

在本次实验中，探究了不同线程数量下，openmp多线程算法的性能表现，其变化趋势大致如下图所示。从图像中可以看出，这三种划分方式的加速比都随着线程数量的增加呈现一个线性增加的趋势，说明openmp的多线程算法具有很好的扩展性。

### 线程管理对比

在本次实验中，探究了动态线程创建和静态线程创建这两种线程管理方式的性能对比。实验结果如图所示。动态线程创建的方式就是在每一轮消元的过程中，重新创建线程，然后进行任务的划分，而静态线程创建的方式则是在最初一次性创建好全部线程，然后在每一轮中重新进行任务的划分。动态线程创建的方式适合于在不同的阶段需要处理完全不同的任务，而静态线程创建的方式适合于各个阶段所需要处理的任务近似相同。因此在本次实验中，更适合采用静态线程创建的方式，而减小线程创建、初始化以及任务划分的额外开销，而这种开销相对于简单的计算而言是比较大的。这也可以从实验数据中得到进行印证。

## x86平台迁移

### 多种SIMD指令集融合

基于前文在ARM平台上对于openmp多线程编程的探究，在本次实验中还将openmp多线程优化方法迁移到x86平台上，做同样的实验探究。由于x86平台上拥有更多的SIMD指令集架构，因此实验中分别探究了SSE、AVX和AVX512三种指令集架构配合openmp多线程的优化效果，测量在不同问题规模下的运行时间，如下表所示。可以看出，openmp多线程可以结合多种SIMD指令集架构，并且在各种指令集架构上的表现基本保持稳定，并没有出现在某种指令集架构下不能够发挥很好的多线程优势的现象。

从数据中可以看出，随着问题规模的增加，SSE、AVX和AVX512三种指令集架构的加速比均保持在一个比较稳定的水平上，这说明向量化的优化已经达到了一个瓶颈，而之所以没有能够达到理论加速比是因为整个程序并不能完全进行向量化的展开，因此其余未能够向量化的部分极大的影响了整体的加速比。

而横向对比同一指令集架构下的SIMD和结合了SIMD的openmp方法，则可以发现，由于在实验中总共拉起了8条线程，因此随着问题规模的增加，当问题规模达到3000时，这三种指令集架构的openmp算法相对于SIMD算法的加速比已经能够逼近8倍了，说明在devcloud平台上，多线程的性能能够得到充分的释放。

### 不同任务划分方式对比

openmp为任务划分提供了三种选项：static、dynamic和guided。static方法是在线程创建完成之后，就明确划分了任务，dynamic方法则是在线程执行的过程中去动态的划分任务，而guided方法则是随着任务的推进逐步缩减任务划分的粒度。在x86平台的实验中，我们对比了这三种方法在数据规模为1000时的性能表现，并将使用VTune性能分析工具对于这三种方式的CPU占用时间进行监测，得到对比图如下。

从图中可以看出，static方式的CPU占用时间分配十分的不均衡，这也体现了这种方法会受到负载不均的制约，因此其运行时间较长，为178.4ms。在本次实验中，dynamic方式的CPU占用时间分布较为均衡，而且通过实验测量程序的运行时间可以发现dynamic方式所消耗的时间最少，为169.2ms。对于guided方法，在本次实验中并没有能够表现出很好的性能，其消耗的时间略高于dynamic方式但仍低于static方式，说明其在一定程度上可以减轻负载不均的影响。

## GPU任务卸载

在本次实验中还尝试了在Devcloud平台上进行GPU运算卸载，将Gauss消元的消元过程卸载到GPU，利用GPU运算单元加速运算，具体代码如下。

在调整数据规模进行多次实验之后，发现offloading方式的性能表现甚至差于普通的串行算法。分析其原因，可能是因为在不同消元轮次之间和每一轮中除法操作和消元操作之间都存在这比较严重的数据依赖，因此会造成GPU各个运算单元之间需要不断地通过CPU进行通信和同步，而这种通信的开销是非常大的。因此当数据规模比较小的时候，会严重的影响性能。笔者猜测这种优化的效果可能会在数据规模很大的情况下，并且尽可能减少数据依赖后体现出来。

# 总结

在本次openmp多线程并行实验中，基于Gauss消元问题，对于过程中的消元操作采用openmp多线程优化，并在线程内进行除法或者减法运算时，结合SIMD向量化处理，在ARM平台上采用openmp+neon的方式，在x86平台上采用openmp+SSE/AVX/AVX512的方式，探究了其并行优化效果。除此之外，还基于cache的空间局部性和时间局部性对比了行划分和列划分的性能差异，基于负载均衡考虑对比了openmp提供的三种schedule方式的性能差异，通过实验可以验证，考虑了cache特性的行划分方式和考虑了负载均衡的动态划分方式均能够取得一定的性能提升。实验还探究了开启线程的数量同优化性能之间的关系，并结合实验平台的硬件参数进行合理假设和分析。在实验的过程中，利用perf和VTune等性能分析工具，对于深层次的内核和硬件事件进行分析，从底层的角度解释了表面上性能差异的原因。本次实验中第一次尝试了通过offloading的方式将部分运算卸载到GPU上进行加速，但是由于数据依赖导致的通信开销，并未能够取得很好的效果，后续会对相关的问题进行进一步研究和优化。